
numpy 

notice how the gradient value is the same as softmax_numpy.py 
gradient value for the first gradient descent is 
  [[ 0.51038765  0.76558147]
 [ 1.02779451  1.54169176]
 [-1.53818216 -2.30727324]]
first pred was 
  [[ 0.255]
 [ 0.514]
 [ 0.231]]
after 500 gradient descent, pred is now 
  [[ 0.001]
 [ 0.001]
 [ 0.999]]
the weights are 
 [[-0.261 -0.341]
 [-0.162 -0.394]
 [ 1.023  1.335]]
 
 
 torch
 
notice how the gradient value is the same as softmax_numpy.py 
gradient value for the first gradient descent is 
 tensor([[ 0.5104,  0.7656],
        [ 1.0278,  1.5417],
        [-1.5382, -2.3073]], dtype=torch.float64)
-----------------------
first pred was 
  tensor([[0.26],
        [0.51],
        [0.23]], dtype=torch.float64, grad_fn=<DivBackward0>)
after 500 gradient descent, pred is now 
  tensor([[5.03e-04],
        [5.22e-04],
        [9.99e-01]], dtype=torch.float64, grad_fn=<DivBackward0>)
the weights are 
 tensor([[-0.26, -0.34],
        [-0.16, -0.39],
        [ 1.02,  1.33]], dtype=torch.float64, requires_grad=True)
