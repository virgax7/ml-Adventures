{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am going to try to answer two questions I had\n",
    "# The example uses logistic classification for determining examples x to be 0 or 1\n",
    "#1. Can theta predict 1 well even though we have prediction of 0's bringing\n",
    "#   the middle theta down and how different is it from updating theta per example vs theta per whole training set?\n",
    "#2. Why does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.array([[0, 1, 0]])\n",
    "zeros = np.repeat(np.array([[1, 1, 0]]).ravel()[None], 3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.vstack((ones.ravel()[None], zeros))\n",
    "y = np.array([[1, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def h(x, theta):\n",
    "    return expit(x.dot(theta.T))\n",
    "\n",
    "def get_cost(x, theta, y):\n",
    "    m = y.shape[1]\n",
    "    term_1 = -y.dot(np.log(h(x, theta)))\n",
    "    term_2 = (1 - y).dot(np.log(1 - h(x, theta)))\n",
    "    return 1 / m * (term_1 - term_2)\n",
    "\n",
    "\n",
    "def get_cost_vector_sum(x_vector, theta, y_indiv):\n",
    "    term_1 = -y_indiv * np.log(h(x_vector, theta))\n",
    "    print(\"#########################  -y_indiv is \", -y_indiv)\n",
    "    print(\"x_vector * theta is \\n\", x_vector * theta)\n",
    "    if (y_indiv[0] == 1):\n",
    "        print(\"np.log(h(x_vector, theta) is \", np.log(h(x_vector, theta)))\n",
    "    term_2 = (1 - y_indiv) * np.log(1 - h(x_vector, theta))\n",
    "    if (y_indiv[0] == 0):\n",
    "        print(\"np.log(1 -h(x_vector, theta) is \", np.log(1 - h(x_vector, theta)))\n",
    "    return term_1 - term_2\n",
    "\n",
    "def get_cost_vector_sum_clean(x_vector, theta, y_indiv):\n",
    "    term_1 = -y_indiv * np.log(h(x_vector, theta))\n",
    "    term_2 = (1 - y_indiv) * np.log(1 - h(x_vector, theta))\n",
    "    return term_1 - term_2\n",
    "\n",
    "\n",
    "def gradient_descent_vector(x_vector, theta, y_indiv):\n",
    "    hx = h(x_vector, theta)\n",
    "    print(\"hx is \", hx)\n",
    "    error = hx - y_indiv\n",
    "    print(\"error is \", error)\n",
    "    print(\"\")\n",
    "    return error.dot(x_vector)\n",
    "\n",
    "def gradient_descent_vector_clean(x_vector, theta, y_indiv):\n",
    "    hx = h(x_vector, theta)\n",
    "    error = hx - y_indiv\n",
    "    return error.dot(x_vector)\n",
    "\n",
    "\n",
    "def gradient_descent(x, theta, y):\n",
    "    m = y.shape[1]\n",
    "    hx = h(x, theta)\n",
    "    error = hx - y.T\n",
    "    return 1 / m * error.T.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_question_indiv():\n",
    "    theta = np.zeros((1, x.shape[1]))\n",
    "    alpha = 1\n",
    "    print(\"initial cost is \", get_cost(x, theta, y))\n",
    "    for i in range(y.shape[1] * repeat):\n",
    "        i %= y.shape[1]\n",
    "        gradient_vector = gradient_descent_vector_clean(x[i][None], theta, y[0][i][None])\n",
    "        theta = theta - alpha * gradient_vector\n",
    "\n",
    "    print(\"\\nfinal cost \", get_cost(x, theta, y))\n",
    "    print(\"----theta is \", theta)\n",
    "    print(\"now printing final cost vectors\")\n",
    "    for i in range(y.shape[1]):\n",
    "        cost_vector_sum = get_cost_vector_sum_clean(x[i][None], theta, y[0][i][None])\n",
    "        hx = h(x[i][None], theta)\n",
    "        print(\"hx is \", hx)\n",
    "        print(\"cost_vector_sum is \", cost_vector_sum)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "\n",
      "final cost  [[0.12250137]]\n",
      "----theta is  [[-3.86226378  0.9171501   0.        ]]\n",
      "now printing final cost vectors\n",
      "hx is  [[0.71446106]]\n",
      "cost_vector_sum is  [[0.33622678]]\n",
      "\n",
      "hx is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n",
      "hx is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n",
      "hx is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_question_indiv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above shows that even though the majority of examples are [1 1 0], it starts converging to a minimum\n",
    "# in terms of the cost function. If I increase repeat, it'll be even close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "\n",
      "final cost  [[0.00125525]]\n",
      "----theta is  [[-13.11287837   5.80198744   0.        ]]\n",
      "now printing final cost vectors\n",
      "hx is  [[0.99698756]]\n",
      "cost_vector_sum is  [[0.00301699]]\n",
      "\n",
      "hx is  [[0.00066778]]\n",
      "cost_vector_sum is  [[0.000668]]\n",
      "\n",
      "hx is  [[0.00066778]]\n",
      "cost_vector_sum is  [[0.000668]]\n",
      "\n",
      "hx is  [[0.00066778]]\n",
      "cost_vector_sum is  [[0.000668]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_question_indiv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, lets try out the whole training set this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_question_whole():\n",
    "    theta = np.zeros((1, x.shape[1]))\n",
    "    alpha = 1\n",
    "    print(\"initial cost is \", get_cost(x, theta, y))\n",
    "    for i in range(y.shape[1] * repeat):\n",
    "        descent = gradient_descent(x, theta, y)\n",
    "        theta = theta - alpha * descent\n",
    "\n",
    "    print(\"\\nfinal cost \", get_cost(x, theta, y))\n",
    "    print(\"----theta is \", theta)\n",
    "    print(\"now printing final cost vectors\")\n",
    "    for i in range(y.shape[1]):\n",
    "        cost_vector_sum = get_cost_vector_sum_clean(x[i][None], theta, y[0][i][None])\n",
    "        hx = h(x[i][None], theta)\n",
    "        print(\"hx is \", hx)\n",
    "        print(\"cost_vector_sum is \", cost_vector_sum)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "\n",
      "final cost  [[0.00126252]]\n",
      "----theta is  [[-13.10091562   5.79700349   0.        ]]\n",
      "now printing final cost vectors\n",
      "hx is  [[0.99697255]]\n",
      "cost_vector_sum is  [[0.00303204]]\n",
      "\n",
      "hx is  [[0.00067245]]\n",
      "cost_vector_sum is  [[0.00067267]]\n",
      "\n",
      "hx is  [[0.00067245]]\n",
      "cost_vector_sum is  [[0.00067267]]\n",
      "\n",
      "hx is  [[0.00067245]]\n",
      "cost_vector_sum is  [[0.00067267]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is for repeat = 1000\n",
    "first_question_whole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "\n",
      "final cost  [[0.13225419]]\n",
      "----theta is  [[-3.63849549  0.90117202  0.        ]]\n",
      "now printing final cost vectors\n",
      "hx is  [[0.71119029]]\n",
      "cost_vector_sum is  [[0.34081524]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is for repeat = 10\n",
    "first_question_whole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so they both work in reducing the cost and better predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the question of why does it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_question_indiv():\n",
    "    theta = np.zeros((1, x.shape[1]))\n",
    "    alpha = 1\n",
    "    print(\"initial cost is \", get_cost(x, theta, y))\n",
    "    print(\"\\n\")\n",
    "    for i in range(y.shape[1] * repeat):\n",
    "        i = i % y.shape[1]\n",
    "        cost_vector_sum = get_cost_vector_sum(x[i][None], theta, y[0][i][None])\n",
    "        gradient_vector = gradient_descent_vector(x[i][None], theta, y[0][i][None])\n",
    "        print(\"theta is \", theta)\n",
    "        print(\"cost_vector_sum is \", cost_vector_sum)\n",
    "        print(\"get_cost is \", get_cost(x, theta, y))\n",
    "        print(\"gradient_vector is \", gradient_vector)\n",
    "        theta = theta - alpha * gradient_vector\n",
    "        print()\n",
    "\n",
    "    print(\"\\nfinal cost \", get_cost(x, theta, y))\n",
    "    print(\"----theta is \", theta)\n",
    "    print(\"now printing final cost vectors\")\n",
    "    for i in range(y.shape[1]):\n",
    "        cost_vector_sum = get_cost_vector_sum_clean(x[i][None], theta, y[0][i][None])\n",
    "        hx = h(x[i][None], theta)\n",
    "        print(\"prediction is \", hx)\n",
    "        print(\"cost_vector_sum is \", cost_vector_sum)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[0. 0. 0.]]\n",
      "np.log(h(x_vector, theta) is  [[-0.69314718]]\n",
      "hx is  [[0.5]]\n",
      "error is  [[-0.5]]\n",
      "\n",
      "theta is  [[0. 0. 0.]]\n",
      "cost_vector_sum is  [[0.69314718]]\n",
      "get_cost is  [[0.69314718]]\n",
      "gradient_vector is  [[ 0.  -0.5  0. ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[0.  0.5 0. ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.97407698]]\n",
      "hx is  [[0.62245933]]\n",
      "error is  [[0.62245933]]\n",
      "\n",
      "theta is  [[0.  0.5 0. ]]\n",
      "cost_vector_sum is  [[0.97407698]]\n",
      "get_cost is  [[0.84907698]]\n",
      "gradient_vector is  [[0.62245933 0.62245933 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-0.62245933 -0.12245933  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.38850402]]\n",
      "hx is  [[0.32192951]]\n",
      "error is  [[0.32192951]]\n",
      "\n",
      "theta is  [[-0.62245933 -0.12245933  0.        ]]\n",
      "cost_vector_sum is  [[0.38850402]]\n",
      "get_cost is  [[0.48044057]]\n",
      "gradient_vector is  [[0.32192951 0.32192951 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-0.94438884 -0.44438884  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.22264738]]\n",
      "hx is  [[0.19960297]]\n",
      "error is  [[0.19960297]]\n",
      "\n",
      "theta is  [[-0.94438884 -0.44438884  0.        ]]\n",
      "cost_vector_sum is  [[0.22264738]]\n",
      "get_cost is  [[0.40194211]]\n",
      "gradient_vector is  [[0.19960297 0.19960297 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.        -0.6439918  0.       ]]\n",
      "np.log(h(x_vector, theta) is  [[-1.06611196]]\n",
      "hx is  [[0.34434475]]\n",
      "error is  [[-0.65565525]]\n",
      "\n",
      "theta is  [[-1.1439918 -0.6439918  0.       ]]\n",
      "cost_vector_sum is  [[1.06611196]]\n",
      "get_cost is  [[0.38254621]]\n",
      "gradient_vector is  [[ 0.         -0.65565525  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.1439918   0.01166345  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.27935903]]\n",
      "hx is  [[0.24373167]]\n",
      "error is  [[0.24373167]]\n",
      "\n",
      "theta is  [[-1.1439918   0.01166345  0.        ]]\n",
      "cost_vector_sum is  [[0.27935903]]\n",
      "get_cost is  [[0.38135239]]\n",
      "gradient_vector is  [[0.24373167 0.24373167 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.38772347 -0.23206822  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.18060336]]\n",
      "hx is  [[0.1652336]]\n",
      "error is  [[0.1652336]]\n",
      "\n",
      "theta is  [[-1.38772347 -0.23206822  0.        ]]\n",
      "cost_vector_sum is  [[0.18060336]]\n",
      "get_cost is  [[0.33942706]]\n",
      "gradient_vector is  [[0.1652336 0.1652336 0.       ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.55295707 -0.39730182  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.13298883]]\n",
      "hx is  [[0.12452513]]\n",
      "error is  [[0.12452513]]\n",
      "\n",
      "theta is  [[-1.55295707 -0.39730182  0.        ]]\n",
      "cost_vector_sum is  [[0.13298883]]\n",
      "get_cost is  [[0.32759181]]\n",
      "gradient_vector is  [[0.12452513 0.12452513 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.         -0.52182695  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.98771925]]\n",
      "hx is  [[0.37242513]]\n",
      "error is  [[-0.62757487]]\n",
      "\n",
      "theta is  [[-1.6774822  -0.52182695  0.        ]]\n",
      "cost_vector_sum is  [[0.98771925]]\n",
      "get_cost is  [[0.325794]]\n",
      "gradient_vector is  [[ 0.         -0.62757487  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.6774822   0.10574792  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.18870504]]\n",
      "hx is  [[0.1719693]]\n",
      "error is  [[0.1719693]]\n",
      "\n",
      "theta is  [[-1.6774822   0.10574792  0.        ]]\n",
      "cost_vector_sum is  [[0.18870504]]\n",
      "get_cost is  [[0.30194638]]\n",
      "gradient_vector is  [[0.1719693 0.1719693 0.       ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.8494515  -0.06622138  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.13736143]]\n",
      "hx is  [[0.12834487]]\n",
      "error is  [[0.12834487]]\n",
      "\n",
      "theta is  [[-1.8494515  -0.06622138  0.        ]]\n",
      "cost_vector_sum is  [[0.13736143]]\n",
      "get_cost is  [[0.28472256]]\n",
      "gradient_vector is  [[0.12834487 0.12834487 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-1.97779637 -0.19456625  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.10787471]]\n",
      "hx is  [[0.10225993]]\n",
      "error is  [[0.10225993]]\n",
      "\n",
      "theta is  [[-1.97779637 -0.19456625  0.        ]]\n",
      "cost_vector_sum is  [[0.10787471]]\n",
      "get_cost is  [[0.27969475]]\n",
      "gradient_vector is  [[0.10225993 0.10225993 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.         -0.29682618  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.8525333]]\n",
      "hx is  [[0.42633353]]\n",
      "error is  [[-0.57366647]]\n",
      "\n",
      "theta is  [[-2.08005631 -0.29682618  0.        ]]\n",
      "cost_vector_sum is  [[0.8525333]]\n",
      "get_cost is  [[0.27971788]]\n",
      "gradient_vector is  [[ 0.         -0.57366647  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.08005631  0.27684028  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.15252204]]\n",
      "hx is  [[0.14146003]]\n",
      "error is  [[0.14146003]]\n",
      "\n",
      "theta is  [[-2.08005631  0.27684028  0.        ]]\n",
      "cost_vector_sum is  [[0.15252204]]\n",
      "get_cost is  [[0.2554607]]\n",
      "gradient_vector is  [[0.14146003 0.14146003 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.22151634  0.13538025  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.11704141]]\n",
      "hx is  [[0.11045164]]\n",
      "error is  [[0.11045164]]\n",
      "\n",
      "theta is  [[-2.22151634  0.13538025  0.        ]]\n",
      "cost_vector_sum is  [[0.11704141]]\n",
      "get_cost is  [[0.24471763]]\n",
      "gradient_vector is  [[0.11045164 0.11045164 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.33196798  0.02492861  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.09490606]]\n",
      "hx is  [[0.09054164]]\n",
      "error is  [[0.09054164]]\n",
      "\n",
      "theta is  [[-2.33196798  0.02492861  0.        ]]\n",
      "cost_vector_sum is  [[0.09490606]]\n",
      "get_cost is  [[0.24136969]]\n",
      "gradient_vector is  [[0.09054164 0.09054164 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.         -0.06561303  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.72649173]]\n",
      "hx is  [[0.48360263]]\n",
      "error is  [[-0.51639737]]\n",
      "\n",
      "theta is  [[-2.42250962 -0.06561303  0.        ]]\n",
      "cost_vector_sum is  [[0.72649173]]\n",
      "get_cost is  [[0.2414697]]\n",
      "gradient_vector is  [[ 0.         -0.51639737  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.42250962  0.45078435  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.13034071]]\n",
      "hx is  [[0.1222037]]\n",
      "error is  [[0.1222037]]\n",
      "\n",
      "theta is  [[-2.42250962  0.45078435  0.        ]]\n",
      "cost_vector_sum is  [[0.13034071]]\n",
      "get_cost is  [[0.22099144]]\n",
      "gradient_vector is  [[0.1222037 0.1222037 0.       ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.54471332  0.32858065  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.10348571]]\n",
      "hx is  [[0.0983111]]\n",
      "error is  [[0.0983111]]\n",
      "\n",
      "theta is  [[-2.54471332  0.32858065  0.        ]]\n",
      "cost_vector_sum is  [[0.10348571]]\n",
      "get_cost is  [[0.21318734]]\n",
      "gradient_vector is  [[0.0983111 0.0983111 0.       ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.64302441  0.23026955  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.08578148]]\n",
      "hx is  [[0.08220523]]\n",
      "error is  [[0.08220523]]\n",
      "\n",
      "theta is  [[-2.64302441  0.23026955  0.        ]]\n",
      "cost_vector_sum is  [[0.08578148]]\n",
      "get_cost is  [[0.21049256]]\n",
      "gradient_vector is  [[0.08220523 0.08220523 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.          0.14806432  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.6218529]]\n",
      "hx is  [[0.5369486]]\n",
      "error is  [[-0.4630514]]\n",
      "\n",
      "theta is  [[-2.72522964  0.14806432  0.        ]]\n",
      "cost_vector_sum is  [[0.6218529]]\n",
      "get_cost is  [[0.21039348]]\n",
      "gradient_vector is  [[ 0.        -0.4630514  0.       ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.72522964  0.61111572  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.11398938]]\n",
      "hx is  [[0.10773257]]\n",
      "error is  [[0.10773257]]\n",
      "\n",
      "theta is  [[-2.72522964  0.61111572  0.        ]]\n",
      "cost_vector_sum is  [[0.11398938]]\n",
      "get_cost is  [[0.19388286]]\n",
      "gradient_vector is  [[0.10773257 0.10773257 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.83296222  0.50338315  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.09288607]]\n",
      "hx is  [[0.08870268]]\n",
      "error is  [[0.08870268]]\n",
      "\n",
      "theta is  [[-2.83296222  0.50338315  0.        ]]\n",
      "cost_vector_sum is  [[0.09288607]]\n",
      "get_cost is  [[0.18786482]]\n",
      "gradient_vector is  [[0.08870268 0.08870268 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.9216649   0.41468046  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.07836161]]\n",
      "hx is  [[0.07536999]]\n",
      "error is  [[0.07536999]]\n",
      "\n",
      "theta is  [[-2.9216649   0.41468046  0.        ]]\n",
      "cost_vector_sum is  [[0.07836161]]\n",
      "get_cost is  [[0.18555863]]\n",
      "gradient_vector is  [[0.07536999 0.07536999 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.          0.33931047  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.53781488]]\n",
      "hx is  [[0.58402302]]\n",
      "error is  [[-0.41597698]]\n",
      "\n",
      "theta is  [[-2.99703489  0.33931047  0.        ]]\n",
      "cost_vector_sum is  [[0.53781488]]\n",
      "get_cost is  [[0.18527311]]\n",
      "gradient_vector is  [[ 0.         -0.41597698  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-2.99703489  0.75528745  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.10099638]]\n",
      "hx is  [[0.09606369]]\n",
      "error is  [[0.09606369]]\n",
      "\n",
      "theta is  [[-2.99703489  0.75528745  0.        ]]\n",
      "cost_vector_sum is  [[0.10099638]]\n",
      "get_cost is  [[0.17204172]]\n",
      "gradient_vector is  [[0.09606369 0.09606369 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.09309859  0.65922376  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.08406203]]\n",
      "hx is  [[0.08062578]]\n",
      "error is  [[0.08062578]]\n",
      "\n",
      "theta is  [[-3.09309859  0.65922376  0.        ]]\n",
      "cost_vector_sum is  [[0.08406203]]\n",
      "get_cost is  [[0.16727184]]\n",
      "gradient_vector is  [[0.08062578 0.08062578 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.17372436  0.57859798  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.07198241]]\n",
      "hx is  [[0.06945274]]\n",
      "error is  [[0.06945274]]\n",
      "\n",
      "theta is  [[-3.17372436  0.57859798  0.        ]]\n",
      "cost_vector_sum is  [[0.07198241]]\n",
      "get_cost is  [[0.16526784]]\n",
      "gradient_vector is  [[0.06945274 0.06945274 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.          0.50914524  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.4706341]]\n",
      "hx is  [[0.62460608]]\n",
      "error is  [[-0.37539392]]\n",
      "\n",
      "theta is  [[-3.2431771   0.50914524  0.        ]]\n",
      "cost_vector_sum is  [[0.4706341]]\n",
      "get_cost is  [[0.16485925]]\n",
      "gradient_vector is  [[ 0.         -0.37539392  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.2431771   0.88453917  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.09034233]]\n",
      "hx is  [[0.08638163]]\n",
      "error is  [[0.08638163]]\n",
      "\n",
      "theta is  [[-3.2431771   0.88453917  0.        ]]\n",
      "cost_vector_sum is  [[0.09034233]]\n",
      "get_cost is  [[0.15416861]]\n",
      "gradient_vector is  [[0.08638163 0.08638163 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.32955873  0.79815754  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.07654195]]\n",
      "hx is  [[0.07368595]]\n",
      "error is  [[0.07368595]]\n",
      "\n",
      "theta is  [[-3.32955873  0.79815754  0.        ]]\n",
      "cost_vector_sum is  [[0.07654195]]\n",
      "get_cost is  [[0.15032453]]\n",
      "gradient_vector is  [[0.07368595 0.07368595 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.40324468  0.72447159  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.06639367]]\n",
      "hx is  [[0.06423759]]\n",
      "error is  [[0.06423759]]\n",
      "\n",
      "theta is  [[-3.40324468  0.72447159  0.        ]]\n",
      "cost_vector_sum is  [[0.06639367]]\n",
      "get_cost is  [[0.14857832]]\n",
      "gradient_vector is  [[0.06423759 0.06423759 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.        0.660234  0.      ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.41655697]]\n",
      "hx is  [[0.65931295]]\n",
      "error is  [[-0.34068705]]\n",
      "\n",
      "theta is  [[-3.46748227  0.660234    0.        ]]\n",
      "cost_vector_sum is  [[0.41655697]]\n",
      "get_cost is  [[0.1481033]]\n",
      "gradient_vector is  [[ 0.         -0.34068705  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.46748227  1.00092105  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.08146591]]\n",
      "hx is  [[0.07823586]]\n",
      "error is  [[0.07823586]]\n",
      "\n",
      "theta is  [[-3.46748227  1.00092105  0.        ]]\n",
      "cost_vector_sum is  [[0.08146591]]\n",
      "get_cost is  [[0.13935294]]\n",
      "gradient_vector is  [[0.07823586 0.07823586 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.54571813  0.92268519  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.07006919]]\n",
      "hx is  [[0.06767069]]\n",
      "error is  [[0.06767069]]\n",
      "\n",
      "theta is  [[-3.54571813  0.92268519  0.        ]]\n",
      "cost_vector_sum is  [[0.07006919]]\n",
      "get_cost is  [[0.13621425]]\n",
      "gradient_vector is  [[0.06767069 0.06767069 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.61338882  0.8550145   0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.06146638]]\n",
      "hx is  [[0.05961544]]\n",
      "error is  [[0.05961544]]\n",
      "\n",
      "theta is  [[-3.61338882  0.8550145   0.        ]]\n",
      "cost_vector_sum is  [[0.06146638]]\n",
      "get_cost is  [[0.13469133]]\n",
      "gradient_vector is  [[0.05961544 0.05961544 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [-1]\n",
      "x_vector * theta is \n",
      " [[-0.          0.79539906  0.        ]]\n",
      "np.log(h(x_vector, theta) is  [[-0.37252934]]\n",
      "hx is  [[0.68898944]]\n",
      "error is  [[-0.31101056]]\n",
      "\n",
      "theta is  [[-3.67300426  0.79539906  0.        ]]\n",
      "cost_vector_sum is  [[0.37252934]]\n",
      "get_cost is  [[0.13418975]]\n",
      "gradient_vector is  [[ 0.         -0.31101056  0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.67300426  1.10640962  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.07399054]]\n",
      "hx is  [[0.07131952]]\n",
      "error is  [[0.07131952]]\n",
      "\n",
      "theta is  [[-3.67300426  1.10640962  0.        ]]\n",
      "cost_vector_sum is  [[0.07399054]]\n",
      "get_cost is  [[0.12692751]]\n",
      "gradient_vector is  [[0.07131952 0.07131952 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.74432378  1.0350901   0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.06446459]]\n",
      "hx is  [[0.06243069]]\n",
      "error is  [[0.06243069]]\n",
      "\n",
      "theta is  [[-3.74432378  1.0350901   0.        ]]\n",
      "cost_vector_sum is  [[0.06446459]]\n",
      "get_cost is  [[0.12433467]]\n",
      "gradient_vector is  [[0.06243069 0.06243069 0.        ]]\n",
      "\n",
      "#########################  -y_indiv is  [0]\n",
      "x_vector * theta is \n",
      " [[-3.80675447  0.97265941  0.        ]]\n",
      "np.log(1 -h(x_vector, theta) is  [[-0.05710945]]\n",
      "hx is  [[0.05550931]]\n",
      "error is  [[0.05550931]]\n",
      "\n",
      "theta is  [[-3.80675447  0.97265941  0.        ]]\n",
      "cost_vector_sum is  [[0.05710945]]\n",
      "get_cost is  [[0.12300421]]\n",
      "gradient_vector is  [[0.05550931 0.05550931 0.        ]]\n",
      "\n",
      "\n",
      "final cost  [[0.12250137]]\n",
      "----theta is  [[-3.86226378  0.9171501   0.        ]]\n",
      "now printing final cost vectors\n",
      "prediction is  [[0.71446106]]\n",
      "cost_vector_sum is  [[0.33622678]]\n",
      "\n",
      "prediction is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n",
      "prediction is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n",
      "prediction is  [[0.04996796]]\n",
      "cost_vector_sum is  [[0.05125957]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what happens to cost, theta and gradients as we run logistic regression\n",
    "second_question_indiv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the cases where -y_indiv is [1], in the first go around, the error which is pred - label is -0.5 \n",
    "# in the second round, it's actually bigger since pred was .34434475 which is farther from 1 then .5\n",
    "# this happened because for the next training examples [1 1 0], [1 1 0], [1 1 0], the labels were 0 and it \n",
    "# decreased the second theta, because it's helping the example classify as 1, whereas the truth(label) is 0\n",
    "\n",
    "# However, look at the third and subsequent rounds, the errors are -0.6275, -0.574, -0.5163, -0.463...-0.311\n",
    "# The reason why is that the first theta, which is a feature of things that are labeled 0 and those not of 1\n",
    "# compensates, and thus our one layer neural net is finding the correlation between the \n",
    "# training set and labels over time even though there are inverse overlaps(the middle theta). \n",
    "\n",
    "# Additionally for more detailed explanation, the first theta gets affected only by training examples of label 0\n",
    "# and thus \"soaks up\" more negative gradients over time in order to counter effect the problem of the theta 2\n",
    "# being increased to correlate the training examples of label 1 to 1\n",
    "# This is evident if you look at the last theta as well ----theta is  [[-3.86226378  0.9171501   0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, let's answer the question of how much using the whole training set to update theta changes things\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_question_whole():\n",
    "    theta = np.zeros((1, x.shape[1]))\n",
    "    alpha = 1\n",
    "    print(\"initial cost is \", get_cost(x, theta, y))\n",
    "    for i in range(y.shape[1] * repeat):\n",
    "        descent = gradient_descent(x, theta, y)\n",
    "        print(\"----theta is \", theta)\n",
    "        theta = theta - alpha * descent\n",
    "\n",
    "    print(\"\\nfinal cost \", get_cost(x, theta, y))\n",
    "    print(\"----theta is \", theta)\n",
    "    print(\"now printing final cost vectors\")\n",
    "    for i in range(y.shape[1]):\n",
    "        cost_vector_sum = get_cost_vector_sum_clean(x[i][None], theta, y[0][i][None])\n",
    "        hx = h(x[i][None], theta)\n",
    "        print(\"hx is \", hx)\n",
    "        print(\"cost_vector_sum is \", cost_vector_sum)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost is  [[0.69314718]]\n",
      "----theta is  [[0. 0. 0.]]\n",
      "----theta is  [[-0.375 -0.25   0.   ]]\n",
      "----theta is  [[-0.63648385 -0.37093973  0.        ]]\n",
      "----theta is  [[-0.83709712 -0.42363149  0.        ]]\n",
      "----theta is  [[-1.00273349 -0.43817988  0.        ]]\n",
      "----theta is  [[-1.14628645 -0.42977653  0.        ]]\n",
      "----theta is  [[-1.27480179 -0.40683679  0.        ]]\n",
      "----theta is  [[-1.39246075 -0.37441346  0.        ]]\n",
      "----theta is  [[-1.50190936 -0.33573083  0.        ]]\n",
      "----theta is  [[-1.60490733 -0.29294051  0.        ]]\n",
      "----theta is  [[-1.70267152 -0.24752573  0.        ]]\n",
      "----theta is  [[-1.79607041 -0.20053277  0.        ]]\n",
      "----theta is  [[-1.88574043 -0.15271133  0.        ]]\n",
      "----theta is  [[-1.97215879 -0.10460373  0.        ]]\n",
      "----theta is  [[-2.05569077 -0.05660394  0.        ]]\n",
      "----theta is  [[-2.13662145 -0.00899782  0.        ]]\n",
      "----theta is  [[-2.21517742  0.03800857  0.        ]]\n",
      "----theta is  [[-2.29154209  0.08426866  0.        ]]\n",
      "----theta is  [[-2.36586651  0.12968056  0.        ]]\n",
      "----theta is  [[-2.4382773   0.17417608  0.        ]]\n",
      "----theta is  [[-2.50888233  0.21771248  0.        ]]\n",
      "----theta is  [[-2.57777501  0.26026625  0.        ]]\n",
      "----theta is  [[-2.64503744  0.30182839  0.        ]]\n",
      "----theta is  [[-2.71074274  0.34240073  0.        ]]\n",
      "----theta is  [[-2.77495688  0.3819932   0.        ]]\n",
      "----theta is  [[-2.83774     0.42062164  0.        ]]\n",
      "----theta is  [[-2.89914746  0.45830618  0.        ]]\n",
      "----theta is  [[-2.95923063  0.49506994  0.        ]]\n",
      "----theta is  [[-3.0180375   0.53093807  0.        ]]\n",
      "----theta is  [[-3.07561314  0.56593696  0.        ]]\n",
      "----theta is  [[-3.13200011  0.60009371  0.        ]]\n",
      "----theta is  [[-3.18723871  0.63343567  0.        ]]\n",
      "----theta is  [[-3.24136727  0.66599009  0.        ]]\n",
      "----theta is  [[-3.29442228  0.69778391  0.        ]]\n",
      "----theta is  [[-3.34643859  0.72884353  0.        ]]\n",
      "----theta is  [[-3.39744953  0.75919474  0.        ]]\n",
      "----theta is  [[-3.44748699  0.78886255  0.        ]]\n",
      "----theta is  [[-3.49658157  0.81787122  0.        ]]\n",
      "----theta is  [[-3.54476258  0.84624413  0.        ]]\n",
      "----theta is  [[-3.59205821  0.87400384  0.        ]]\n",
      "\n",
      "final cost  [[0.13225419]]\n",
      "----theta is  [[-3.63849549  0.90117202  0.        ]]\n",
      "now printing final cost vectors\n",
      "hx is  [[0.71119029]]\n",
      "cost_vector_sum is  [[0.34081524]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n",
      "hx is  [[0.06080658]]\n",
      "cost_vector_sum is  [[0.06273383]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_question_whole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end results are pretty similar\n",
    "# when examining theta, I'm going to view the thetas from the individuals update every four times(set size)\n",
    "# vs the whole training set\n",
    "\n",
    "#indv set\n",
    "#theta is  [[0. 0. 0.]]\n",
    "#theta is  [[-1.1439918 -0.6439918  0.       ]]\n",
    "#theta is  [[-1.6774822  -0.52182695  0.        ]]\n",
    "#theta is  [[-2.08005631 -0.29682618  0.        ]]\n",
    "#theta is  [[-2.42250962 -0.06561303  0.        ]]\n",
    "#theta is  [[-2.72522964  0.14806432  0.        ]]\n",
    "#theta is  [[-2.99703489  0.33931047  0.        ]]\n",
    "#theta is  [[-3.2431771   0.50914524  0.        ]]\n",
    "#theta is  [[-3.46748227  0.660234    0.        ]]\n",
    "#theta is  [[-3.67300426  0.79539906  0.        ]]\n",
    "\n",
    "\n",
    "#whole set\n",
    "#----theta is  [[0. 0. 0.]]\n",
    "#----theta is  [[-0.375 -0.25   0.   ]]\n",
    "#----theta is  [[-0.63648385 -0.37093973  0.        ]]\n",
    "#----theta is  [[-0.83709712 -0.42363149  0.        ]]\n",
    "#----theta is  [[-1.00273349 -0.43817988  0.        ]]\n",
    "#----theta is  [[-1.14628645 -0.42977653  0.        ]]\n",
    "#----theta is  [[-1.27480179 -0.40683679  0.        ]]\n",
    "#----theta is  [[-1.39246075 -0.37441346  0.        ]]\n",
    "#----theta is  [[-1.50190936 -0.33573083  0.        ]]\n",
    "#----theta is  [[-1.60490733 -0.29294051  0.        ]]\n",
    "#----theta is  [[-1.70267152 -0.24752573  0.        ]]\n",
    "#----theta is  [[-1.79607041 -0.20053277  0.        ]]\n",
    "#----theta is  [[-1.88574043 -0.15271133  0.        ]]\n",
    "#----theta is  [[-1.97215879 -0.10460373  0.        ]]\n",
    "#----theta is  [[-2.05569077 -0.05660394  0.        ]]\n",
    "#----theta is  [[-2.13662145 -0.00899782  0.        ]]\n",
    "#----theta is  [[-2.21517742  0.03800857  0.        ]]\n",
    "#----theta is  [[-2.29154209  0.08426866  0.        ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, it took going through and updating theta using batch(whole training set), many more times that\n",
    "# just going through and updating theta each time you encounter an example.\n",
    "# this is because theta is being updated by a more weighted gradient and since there is more examples with\n",
    "# 0 vs 1, the middle theta's update to a positive direction is weighted down by the overall sum of \n",
    "# the update for the training set, which is initially wanting it go in the negative direction since there are\n",
    "# more 1's. However, again, once the first theta \"soaks up\" enough of the negative gradients over time, the neural net\n",
    "# can once again, start changing the middle theta to a positive direction since the loss is eventually bigger\n",
    "# for the middle theta, if it stays negative, since the loss is smaller for the first theta since it got updated\n",
    "# so in the negative direction.\n",
    "\n",
    "# this concludes that both individual updates and batch updates work. Here is the last theta of the batch update\n",
    "#----theta is  [[-3.59205821  0.87400384  0.        ]] \n",
    "# vs the last theta of the individual update\n",
    "#----theta is  [[-3.86226378  0.9171501   0.        ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
